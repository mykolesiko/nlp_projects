{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "Копия блокнота \"Копия блокнота \"Lab01_Poetry_generation.ipynb\"\"",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6lrNmmtqNg7"
      },
      "source": [
        "## Lab 01. Poetry generation\n",
        "\n",
        "Let's try to generate some poetry using RNNs. \n",
        "\n",
        "You have several choices here: \n",
        "\n",
        "* The Shakespeare sonnets, file `sonnets.txt` available in the notebook directory.\n",
        "\n",
        "* Роман в стихах \"Евгений Онегин\" Александра Сергеевича Пушкина. В предобработанном виде доступен по [ссылке](https://github.com/attatrol/data_sources/blob/master/onegin.txt).\n",
        "\n",
        "* Some other text source, if it will be approved by the course staff.\n",
        "\n",
        "Text generation can be designed in several steps:\n",
        "    \n",
        "1. Data loading.\n",
        "2. Dictionary generation.\n",
        "3. Data preprocessing.\n",
        "4. Model (neural network) training.\n",
        "5. Text generation (model evaluation).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46afKU7UqNg9"
      },
      "source": [
        "import string\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSo8VsIoqNg-"
      },
      "source": [
        "### Data loading: Shakespeare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii3Cr6urqNg_"
      },
      "source": [
        "Shakespeare sonnets are awailable at this [link](http://www.gutenberg.org/ebooks/1041?msg=welcome_stranger). In addition, they are stored in the same directory as this notebook (`sonnetes.txt`). Simple preprocessing is already done for you in the next cell: all technical info is dropped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "M528WG1BqNg_"
      },
      "source": [
        "#if not os.path.exists('sonnets.txt'):\n",
        "#    !wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/master/homeworks_basic/Lab2_DL/sonnets.txt\n",
        "\n",
        "#with open('sonnets.txt', 'r') as iofile:\n",
        " #   text = iofile.readlines()\n",
        "    \n",
        "#TEXT_START = 45\n",
        "#TEXT_END = -368\n",
        "#text = text[TEXT_START : TEXT_END]\n",
        "#assert len(text) == 2616"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mWm1rYTqNhA"
      },
      "source": [
        "In opposite to the in-class practice, this time we want to predict complex text. Let's reduce the complexity of the task and lowercase all the symbols.\n",
        "\n",
        "Now variable `text` is a list of strings. Join all the strings into one and lowercase it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjmGOW2dqNhA"
      },
      "source": [
        "import re\n",
        "SOS = '<'\n",
        "EOS = '>'\n",
        "\n",
        "# Join all the strings into one and lowercase it\n",
        "# Put result into variable text.\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "\n",
        "def get_poems_list(text):\n",
        "  tokenizer = TweetTokenizer()\n",
        "  preprocess = lambda line: \" \".join(tokenizer.tokenize(line.lower()))\n",
        "  sonets = [\"\"]\n",
        "  i = 0\n",
        "  new_sonet = 1\n",
        "  for line in text:\n",
        "    #print(line)\n",
        "    if (len(line) < 10) or (line == '\\n'):\n",
        "      if new_sonet == 0:\n",
        "        sl = list(sonets[i])\n",
        "        sl[-1] = EOS\n",
        "        sonets[i] = \"\".join(sl)\n",
        "        sonets.append(\"\")\n",
        "        i += 1\n",
        "        new_sonet = 1\n",
        "      continue\n",
        "    if new_sonet == 1:\n",
        "      sonets[i] += SOS \n",
        "      sonets[i] += \" \" \n",
        "    else:  \n",
        "      sonets[i]  += \"* \"\n",
        "    new_sonet = 0\n",
        "    line = line.strip()\n",
        "    #line = preprocess(line)\n",
        "    re.sub(r'\\s+', ' ', line)\n",
        "    line += \"\\n\"\n",
        "  \n",
        "    sonets[i] += line.lower()\n",
        "  return sonets  \n",
        "  \n",
        "#all_string = \"\"\n",
        "#for line in text:\n",
        "#  all_string += line\n",
        "\n",
        "#text_all = all_string.lower()\n",
        "\n",
        "#assert len(text_all) == 100225, 'Are you sure you have concatenated all the strings?'\n",
        "#assert not any([x in set(text_all) for x in string.ascii_uppercase]), 'Uppercase letters are present'\n",
        "#print('OK!')"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfzhbCmbqNhA"
      },
      "source": [
        "### Data loading: \"Евгений Онегин\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frPEe2kgqNhB",
        "outputId": "21e17257-2b80-4ac6-a4f6-f2b8f062a419"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/attatrol/data_sources/master/onegin.txt\n",
        "    \n",
        "with open('onegin.txt', 'r') as iofile:\n",
        "    text_onegin = iofile.readlines()\n",
        "    \n",
        "text_onegin = [x.replace('\\t\\t', '') for x in text_onegin]\n",
        "text_onegin_new = \" \".join(text_onegin)\n",
        "print(text_onegin_new[0:300])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-28 07:43:25--  https://raw.githubusercontent.com/attatrol/data_sources/master/onegin.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 262521 (256K) [text/plain]\n",
            "Saving to: ‘onegin.txt.1’\n",
            "\n",
            "onegin.txt.1        100%[===================>] 256.37K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-04-28 07:43:26 (7.08 MB/s) - ‘onegin.txt.1’ saved [262521/262521]\n",
            "\n",
            "\n",
            " I\n",
            " \n",
            " «Мой дядя самых честных правил,\n",
            " Когда не в шутку занемог,\n",
            " Он уважать себя заставил\n",
            " И лучше выдумать не мог.\n",
            " Его пример другим наука;\n",
            " Но, боже мой, какая скука\n",
            " С больным сидеть и день и ночь,\n",
            " Не отходя ни шагу прочь!\n",
            " Какое низкое коварство\n",
            " Полуживого забавлять,\n",
            " Ему подушки поправлять\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX8sEVKqCDDO"
      },
      "source": [
        "poems = get_poems_list(text_onegin)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf09M7UHC0uO",
        "outputId": "9439bfde-189f-4b51-c75e-67d455e79b43"
      },
      "source": [
        "print(poems[60])"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< от хладного разврата света\n",
            "* еще увянуть не успев,\n",
            "* его душа была согрета\n",
            "* приветом друга, лаской дев;\n",
            "* он сердцем милый был невежда,\n",
            "* его лелеяла надежда,\n",
            "* и мира новый блеск и шум\n",
            "* еще пленяли юный ум.\n",
            "* он забавлял мечтою сладкой\n",
            "* сомненья сердца своего;\n",
            "* цель жизни нашей для него\n",
            "* была заманчивой загадкой,\n",
            "* над ней он голову ломал\n",
            "* и чудеса подозревал.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdQyxyC_EMcF"
      },
      "source": [
        "all_string = \"\"\n",
        "for line in text_onegin:\n",
        "  all_string += line\n",
        "\n",
        "text_all = all_string.lower()"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV36OBQRqNhC"
      },
      "source": [
        "In opposite to the in-class practice, this time we want to predict complex text. Let's reduce the complexity of the task and lowercase all the symbols.\n",
        "\n",
        "Now variable `text` is a list of strings. Join all the strings into one and lowercase it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9To3OdnqNhD",
        "outputId": "d6e8a3cd-e9af-457e-b4a0-f13aee66296e"
      },
      "source": [
        "PAD = '_'\n",
        "BEGIN_LINE = '*'\n",
        "out = text_all\n",
        "tokens = sorted(set(out))\n",
        "tokens.append(SOS)\n",
        "tokens.append(EOS)\n",
        "tokens.append(BEGIN_LINE)\n",
        "tokens.append(PAD)\n",
        "print(tokens)\n",
        "num_tokens = len(tokens)\n",
        "print(num_tokens)\n",
        "\n",
        "#PAD_INDEX = 40\n",
        "#PAD_INDEX = 86"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '(', ')', ',', '-', '.', '5', '7', '8', '9', ':', ';', '?', '[', ']', '^', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '«', '»', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё', '–', '—', '’', '…', '€', '<', '>', '*', '_']\n",
            "87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhySEJqAHqSe",
        "outputId": "e66acc01-4a96-4390-e98b-f605ac54ca66"
      },
      "source": [
        "num_tokens = len(tokens)\n",
        "print(num_tokens)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJI59E2vkduA",
        "outputId": "9fd64334-ad8d-417b-deea-09823c0142a2"
      },
      "source": [
        "len_poems = list(map(len, poems))\n",
        "print(max(len_poems))\n",
        "print(min(len_poems))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1625\n",
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY7YZlZVqNhD"
      },
      "source": [
        "Create dictionary `token_to_idx = {<char>: <index>}` and dictionary `idx_to_token = {<index>: <char>}`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "CZDpy4Y-qNhD"
      },
      "source": [
        "# dict <index>:<char>\n",
        "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
        "\n",
        "# dict <char>:<index>\n",
        "idx_to_token = {idx: token for idx, token in enumerate(tokens)}"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE3mjUXVqNhE"
      },
      "source": [
        "*Comment: in this task we have only 38 different tokens, so let's use one-hot encoding.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El9flGUsqNhE"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2OGpAXBqNhF"
      },
      "source": [
        "Now we want to build and train recurrent neural net which would be able to something similar to Shakespeare's poetry.\n",
        "\n",
        "Let's use vanilla RNN, similar to the one created during the lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrOgtki91Hfd"
      },
      "source": [
        "Первая модель - самописная vanilla-RNN. Токенами являются буквы, на вход модели подаются отдельные стихи, предсказывается следующая буква.\n",
        "При подсчете лосса pad-символы игнорируются (в случае стихов разной длины в батче).\n",
        "Подбираем следующие гиперпараметры модели - размер скрытого слоя = 300, размер выходного слоя 0  - количество букв (87), размер embedding слоя возьмем немного излишним, округлим до 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dIUmGGREqNhF"
      },
      "source": [
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtOduMl-tAxf"
      },
      "source": [
        "def to_matrix(sentences, max_len = None, pad=PAD, dtype='int32', batch_first=True):\n",
        "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
        "    \n",
        "    max_len = max_len or max(map(len, sentences))\n",
        "    sentences_ix = np.zeros([len(sentences), max_len], dtype) + token_to_id[PAD]\n",
        "    #sentences_ix = np.full((len(sentences), max_len), ord(pad), dtype) \n",
        "    #print(sentences_ix)\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        line_ix = [token_to_id[c] for c in sentences[i]]\n",
        "        sentences_ix[i, :len(line_ix)] = line_ix[:max_len]\n",
        "        \n",
        "    if not batch_first: # convert [batch, time] into [time, batch]\n",
        "        sentences_ix = np.transpose(sentences_ix)\n",
        "    #print(sentences_ix)\n",
        "    return sentences_ix"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPL-ke4OqNhF"
      },
      "source": [
        "Plot the loss function (axis X: number of epochs, axis Y: loss function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WmIRJoyiqNhG"
      },
      "source": [
        "class CharRNNCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement the scheme above as torch module\n",
        "    \"\"\"\n",
        "    def __init__(self, num_tokens=len(tokens), embedding_size=100, rnn_num_units = 300):\n",
        "        super(self.__class__,self).__init__()\n",
        "        self.num_units = rnn_num_units\n",
        "        \n",
        "        self.embedding = nn.Embedding(num_tokens, embedding_size)\n",
        "        self.rnn_update = nn.Linear(\n",
        "            embedding_size + rnn_num_units,\n",
        "            rnn_num_units\n",
        "        )\n",
        "        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
        "        \n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"\n",
        "        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n",
        "        We'll call it repeatedly to produce the whole sequence.\n",
        "        \n",
        "        :param x: batch of character ids, containing vector of int64\n",
        "        :param h_prev: previous rnn hidden states, containing matrix [batch, rnn_num_units] of float32\n",
        "        \"\"\"\n",
        "        # get vector embedding of x\n",
        "        #print(\"before\")\n",
        "        #print(x)\n",
        "        x_emb = self.embedding(x)\n",
        "        #print(f\"***{x.shape} {x}\")\n",
        "        #print(x_emb)\n",
        "        #print(h_prev.shape)\n",
        "        \n",
        "        # compute next hidden state using self.rnn_update\n",
        "        # hint: use torch.cat(..., dim=...) for concatenation\n",
        "        x_and_h = torch.cat([x_emb, h_prev], dim=-1)# YOUR CODE HERE\n",
        "        #print(x_and_h.shape)\n",
        "        h_next = self.rnn_update(x_and_h) # YOUR CODE HERE\n",
        "        h_next = torch.tanh(h_next) # YOUR CODE HERE\n",
        "        assert h_next.size() == h_prev.size()\n",
        "        \n",
        "        #compute logits for next character probs\n",
        "        logits = self.rnn_to_logits(h_next) # YOUR CODE\n",
        "        \n",
        "        return h_next, F.log_softmax(logits, -1)\n",
        "    \n",
        "    def initial_state(self, batch_size):\n",
        "        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n",
        "        return torch.zeros(batch_size, self.num_units, requires_grad=True)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dN4XU5AeUJP"
      },
      "source": [
        "char_rnn = CharRNNCell()\n",
        "criterion = nn.NLLLoss(ignore_index = token_to_id[PAD]) # YOUR CODE HERE"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEKBfsLbeU9F"
      },
      "source": [
        "def rnn_loop(char_rnn, batch_ix):\n",
        "    \"\"\"\n",
        "    Computes log P(next_character) for all time-steps in names_ix\n",
        "    :param names_ix: an int32 matrix of shape [batch, time], output of to_matrix(names)\n",
        "    \"\"\"\n",
        "    batch_size, max_length = batch_ix.size()\n",
        "    hid_state = char_rnn.initial_state(batch_size)\n",
        "    logprobs = []\n",
        "\n",
        "    for x_t in batch_ix.transpose(0,1):\n",
        "        #print(x_t)\n",
        "        hid_state, logp_next = char_rnn(x_t, hid_state)  # <-- here we call your one-step code\n",
        "        logprobs.append(logp_next)\n",
        "        \n",
        "    return torch.stack(logprobs, dim=1)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDreOnHaenuV",
        "outputId": "d3f7a7d5-a622-4a41-b534-a58b01dc9439"
      },
      "source": [
        "#batch_ix = to_matrix(sonets)\n",
        "batch_ix = to_matrix(poems)\n",
        "batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "\n",
        "logp_seq = rnn_loop(char_rnn, batch_ix)\n",
        "\n",
        "predictions_logp = logp_seq[:, :-1]\n",
        "actual_next_tokens = batch_ix[:, 1:]\n",
        "#print(predictions_logp.shape)\n",
        "#print(actual_next_tokens.shape)\n",
        "# .contiguous() method checks that tensor is stored in the memory correctly to \n",
        "# get its view of desired shape.\n",
        "\n",
        "loss = criterion(predictions_logp.contiguous().view(-1, num_tokens), \n",
        "                  actual_next_tokens.contiguous().view(-1))\n",
        "print(loss)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(4.4642, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4Ki6XHPzf5F"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from random import sample\n",
        "\n",
        "char_rnn = CharRNNCell()\n",
        "criterion = nn.NLLLoss(ignore_index = token_to_id[PAD])\n",
        "#opt = torch.optim.Adam(char_rnn.parameters(), )\n",
        "history = []"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT3sInCsE5DD"
      },
      "source": [
        "sonets = poems.copy()"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "mINkUg2hvq1K",
        "outputId": "ac00ce97-85d0-4095-9ac2-cbf2b01c7c85"
      },
      "source": [
        "import pickle\n",
        "\n",
        "opt = torch.optim.Adam(char_rnn.parameters(), lr=1e-4, amsgrad=True)\n",
        "#opt = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 100, 1)\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "#    opt, \n",
        "#    patience=10, \n",
        "#    verbose=True, \n",
        "#    factor=0.5,\n",
        "#    threshold = 0.0001\n",
        "#)\n",
        "\n",
        "for i in range(5000):\n",
        "    batch_ix = to_matrix(sample(sonets, 32))\n",
        "    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "    logp_seq = rnn_loop(char_rnn, batch_ix)\n",
        "    # compute loss\n",
        "    predictions_logp = logp_seq[:, :-1]\n",
        "    actual_next_tokens = batch_ix[:, 1:]\n",
        "    loss = criterion(predictions_logp.contiguous().view(-1, num_tokens), \n",
        "                  actual_next_tokens.contiguous().view(-1))\n",
        "    #print(loss.item())\n",
        "    # train with backprop\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "    loss_value = loss.data.numpy()\n",
        "    scheduler.step(loss_value)\n",
        "    # YOUR CODE HERE\n",
        "    history.append(loss.data.numpy())\n",
        "    if (i+1)%100==0:\n",
        "        clear_output(True)\n",
        "        plt.plot(history,label='loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    if (i+1)%500==0:\n",
        "        with open(f\"/content/drive/MyDrive/MADE/semester2/NLP/char_rnn_onegin{loss_value}.pth\", \"wb\") as fp:\n",
        "                 torch.save(model.state_dict(), fp)\n",
        "        #with open(f\"/content/drive/MyDrive/MADE/semester2/NLP/char_rnn_onegin{loss_value}.pkl\", \"wb\") as fp:\n",
        "         #   pickle.dump(char_rnn, fp)   \n",
        "\n",
        "\n",
        "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-146-f35c8dc2c961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#print(loss.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# train with backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX6kCgQ8zwTg"
      },
      "source": [
        "Минимальное значение лосса получаем, меняя оптизмайзеры c Adam и ReduceLROnPlateau в начале, на SGD в конце. Пробовала подключить шедьюлер CosineAnnealingWarmRestarts, который периодически меняет значение lr от максимального до минимального (период растет), когда лосс совсем перестал убывать. Дало небольшое уменьшение лосса.\n",
        "В результате минимальное полученное значение лосса 0.54.\n",
        "\n",
        "Так же заметила, что чем более сложная сеть, например чем больше размер hidden слоя, тем меньший лосс удается получить."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXPh6S3jNCl1"
      },
      "source": [
        "#with open(\"/content/drive/MyDrive/MADE/semester2/NLP/char_rnn_onegin.pkl\", \"wb\") as fp:\n",
        "#             pickle.dump(char_rnn, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64nmf4_hIpJE"
      },
      "source": [
        "model = CharRNNCell()\n",
        "with open(\"/content/drive/MyDrive/MADE/semester2/NLP/char_rnn_onegin0.5484821200370789.pth\", \"rb\") as fp:\n",
        "    best_state_dict = torch.load(fp, map_location=\"cpu\")\n",
        "    model.load_state_dict(best_state_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBBPCbCc5AlS"
      },
      "source": [
        "MAX_LENGTH = 300"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqBr4kL0f3ej",
        "outputId": "4eaa7c47-aecf-4217-c9b6-188994a98122"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRYg5A9lqNhH"
      },
      "source": [
        "def generate_sample(char_rnn, seed_phrase='  ', max_length= MAX_LENGTH, temperature=1.0):\n",
        "    '''\n",
        "    ### Disclaimer: this is an example function for text generation.\n",
        "    ### You can either adapt it in your code or create your own function\n",
        "    \n",
        "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
        "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
        "    :param max_length: maximum output length, including seed_phrase\n",
        "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs, \n",
        "        smaller temperature converges to the single most likely output.\n",
        "        \n",
        "    Be careful with the model output. This model waits logits (not probabilities/log-probabilities)\n",
        "    of the next symbol.\n",
        "    '''\n",
        "    \n",
        "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
        "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n",
        "    hid_state = char_rnn.initial_state(batch_size=1)\n",
        "    \n",
        "    #feed the seed phrase, if any\n",
        "    for i in range(len(seed_phrase) - 1):\n",
        "        #print(x_sequence[:, -1].shape, hid_state.shape)\n",
        "        hid_state, out = char_rnn(x_sequence[:, i], hid_state)\n",
        "    \n",
        "    #start generating\n",
        "    for _ in range(max_length - len(seed_phrase)):\n",
        "        #print(x_sequence.shape, x_sequence, hid_state.shape)\n",
        "        hid_state, out = char_rnn(x_sequence[:, -1], hid_state)\n",
        "        # Be really careful here with the model output\n",
        "        p_next = F.softmax(out / temperature, dim=-1).data.numpy()[0]\n",
        "        \n",
        "        # sample next token and push it back into x_sequence\n",
        "        #print(p_next.shape, len(tokens))\n",
        "        next_ix = np.random.choice(len(tokens), p=p_next)\n",
        "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
        "        #print(x_sequence.shape, next_ix.shape)\n",
        "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
        "    result = ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])\n",
        "    #result.replace(0, \"\")    \n",
        "    return result"
      ],
      "execution_count": 359,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7otqdBLD4RMM"
      },
      "source": [
        "Генерируем тексты для различных температур и модели с минимальным лоссом. Уменьшение температуры делает распределение следующей буквы наиболее однозначно выделяя определенную букву, а увеличение температуры в пределе стремится сделать вероятность всех букв одинаковой. Таким образом, повышая температуру мы получаем менее вероятные слова, иногда не существующие."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpYonS6sqNhI"
      },
      "source": [
        "# An example of generated text.\n",
        "result = generate_sample(char_rnn, seed_phrase='< как часто летнею порою\\n', max_length= 1000, temperature=0.1)"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JLRjOhE7m9l"
      },
      "source": [
        "Температура 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKbjjYC8huFx",
        "outputId": "54b6f9fb-5a3e-41b6-bd55-9af746005d51"
      },
      "source": [
        "ind = result.find('>')\n",
        "print(result[0: ind + 1])"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< как часто летнею порою\n",
            "* и все круживая стеклася\n",
            "* я безменял сердце ей.\n",
            "* корстизо. но любит голос, —\n",
            "* то счалася греметь его.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI2tbfij7lSC"
      },
      "source": [
        "Температура 0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZbEvMndBHqK",
        "outputId": "0d368565-04f4-4513-dcc6-d268e55aac3a"
      },
      "source": [
        "result = (generate_sample(char_rnn, seed_phrase='< как часто летнею порою\\n', max_length= 1000, temperature=0.5))\n",
        "ind = result.find('>')\n",
        "print(result[0: ind + 1])\n"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< как часто летнею порою\n",
            "* и все круживая восквель,\n",
            "* она громит в ней най доброй вновь\n",
            "* расстаться на руках свой пирог\n",
            "* и лицо братяе моды,\n",
            "* в окно слышит о сенегам;\n",
            "* с девочкой неосподован\n",
            "* вдруг он слачки актары\n",
            "* в душою видел он\n",
            "* и страстей и утрежною, —\n",
            "* всё тридцати значрого стихах\n",
            "* я не слушает молодых,\n",
            "* открыть преданный брик,\n",
            "* без милой одесовай);\n",
            "* но скоти светскою не доставе\n",
            "* с хетушка, так уж и нежной,\n",
            "* как жельчиным слова народы\n",
            "* ту, чтобный ветреное привляндана\n",
            "* татьяна бедная уши,\n",
            "* и гордость моего романа,\n",
            "* в саду… створен. милой своей пиром,\n",
            "* итешен и громать его,\n",
            "* а было тайнын чувству открыль,\n",
            "* и с ним покамест, ни силь разлоя\n",
            "* света шесная, за них день\n",
            "* его одной. так же шумной\n",
            "* обноврички моя, весна жаль.\n",
            "* среди проводами сы вот\n",
            "* мелькали кловенной след,\n",
            "* всё тот же друг молодой век.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsEFPn4k7uvT"
      },
      "source": [
        "Температура 0.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llzpcznwBzal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c347af30-43c7-4a68-f375-5352c50ebeb6"
      },
      "source": [
        "result = generate_sample(char_rnn, seed_phrase='< мне памятно другое время!\\n', max_length= 700, temperature=0.2)\n",
        "ind = result.find('>')\n",
        "print(result[0: ind + 1])"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< мне памятно другое время!\n",
            "* в моднеть умел ваш гранит,\n",
            "* и так подали за нею;\n",
            "* страхам должна в том соседин,\n",
            "* и в сад подня был он мне —\n",
            "* всё душу те он возмужить,\n",
            "* летают рожда тайно гоним.\n",
            "* всегда я не спешит,\n",
            "* к нескобна уж как ранний,\n",
            "* души могла, покой себя,\n",
            "* о совсем летер и нет най.\n",
            "* он мне над озерка умом;\n",
            "* онегин, высоком других\n",
            "* для недогнеми бесела:\n",
            "* сперший образец... и послепет,\n",
            "* в костенит на устаруж,\n",
            "* но полно, шалости вервим\n",
            "* достаянь матушки теревой).\n",
            "* всё тет и на легкой пустой.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD2-DU6N7xnT"
      },
      "source": [
        "Тексты получились так себе, структура текста есть, но рифма и слог стиха не прослеживаются, также многие слова получаются даже в случае низкой температуры несуществующие"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMbHNfOSGUlU"
      },
      "source": [
        "char_rnn = CharRNNCell()\n",
        "criterion = nn.NLLLoss(ignore_index = token_to_id[PAD])\n",
        "#opt = torch.optim.Adam(char_rnn.parameters(), )\n",
        "history = []"
      ],
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMoH9KiXIGhV"
      },
      "source": [
        "Возможно то, что не получается получить лосс с меньшим значением связано еще и с проблемой затухающих градиентов, от которой rnn не застрахована.\n",
        "Попробуем уменьшить длину последовательности до 100, подаваемую модели."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "3bmSNNvPFJeu",
        "outputId": "2b027e4d-9f1e-4a7b-d33f-1118c0f6c353"
      },
      "source": [
        "opt = torch.optim.Adam(char_rnn.parameters(), lr=1e-3, amsgrad=True)\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 100, 1)\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "#    opt, \n",
        "#    patience=10, \n",
        "#    verbose=True, \n",
        "#    factor=0.5,\n",
        "#    threshold = 0.0001\n",
        "#)\n",
        "\n",
        "for i in range(5000):\n",
        "    batch_ix = to_matrix(sample(sonets, 32), max_len = 400)\n",
        "    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "    logp_seq = rnn_loop(char_rnn, batch_ix)\n",
        "    # compute loss\n",
        "    predictions_logp = logp_seq[:, :-1]\n",
        "    actual_next_tokens = batch_ix[:, 1:]\n",
        "    loss = criterion(predictions_logp.contiguous().view(-1, num_tokens), \n",
        "                  actual_next_tokens.contiguous().view(-1))\n",
        "    #print(loss.item())\n",
        "    # train with backprop\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "    loss_value = loss.data.numpy()\n",
        "    scheduler.step(loss_value)\n",
        "    # YOUR CODE HERE\n",
        "    history.append(loss.data.numpy())\n",
        "    if (i+1)%100==0:\n",
        "        clear_output(True)\n",
        "        plt.plot(history,label='loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    if (i+1)%500==0:\n",
        "        with open(f\"/content/drive/MyDrive/MADE/semester2/NLP/char_rnn_onegin{loss_value}.pth\", \"wb\") as fp:\n",
        "                 torch.save(model.state_dict(), fp)\n",
        "        #with open(f\"/content/drive/MyDrive/MADE/semester2/NLP/char_rnn_onegin{loss_value}.pkl\", \"wb\") as fp:\n",
        "         #   pickle.dump(char_rnn, fp)   \n",
        "\n",
        "\n",
        "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\"\n"
      ],
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvkfDfXkIYnZ",
        "outputId": "7b0e672c-26f2-4545-af72-acea60293307"
      },
      "source": [
        "print(history[-1])"
      ],
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.086693294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBCR6n4uIij5"
      },
      "source": [
        "with open(f\"/content/drive/MyDrive/MADE/semester2/NLP/char_rnn_onegin{history[-1]}.pth\", \"wb\") as fp:\n",
        "                 torch.save(model.state_dict(), fp)"
      ],
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZHzFTULIpRy"
      },
      "source": [
        "Лосс получился крайне маленький. Посмотрим, что напредсказывает модель. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XC9NdyWJaP5",
        "outputId": "d703a78e-e8ef-4afa-89bd-25e626a0856d"
      },
      "source": [
        "result = generate_sample(char_rnn, seed_phrase='< мне памятно другое время!\\n', max_length= 400, temperature=0.5)\n",
        "print(result)\n",
        "#ind = result.find('>')\n",
        "#print(result[0: ind + 1])"
      ],
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< мне памятно другое время!\n",
            "* в заветных иногда мечтах\n",
            "* держу я счастливое стремя…\n",
            "* и ножку чувства разделой,\n",
            "* порой ленянный жежал…\n",
            "* к немушел ко льбомы, черадиной\n",
            "* как от ужал толной, и ты,\n",
            "* а тепок уж он тревают:\n",
            "* блажут, и дой небеды, в столе,\n",
            "* строфа, коченной жежен\n",
            "* зы глуши натосле на мною.\n",
            "* вот это барадки не моей.\n",
            "* героев наших разлет:\n",
            "* в нему нас печальной девець.\n",
            "* любви бы \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDPCrCV1Ka4C"
      },
      "source": [
        "Видим, что моделька предсказала ровно 100 символов с точностью 100 процентов. А больше она предсказывать не умеет.\n",
        "\n",
        "< мне памятно другое время!\n",
        "\n",
        " в заветных иногда мечтах\n",
        "\n",
        " держу я счастливое стремя…\n",
        "\n",
        " и ножку чувства\n",
        "\n",
        " Причем таких стишков по 100 символов не так уж и много при данном способе формирования батча (одна строчка в батче - один стих, стихов достаточно немного) поэтому тем более произошло переобучение\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV3evwW86j0y"
      },
      "source": [
        "Можно было бы обучать на последовательности определенной длины < 200, двигая данное окошко по всей последовательности символов из всех стихов. Тогда бы таких примеров было больше и не было бы такого переобучения. Реализуем это уже в новой сети LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sq8NYnfqNhJ"
      },
      "source": [
        "### More poetic model\n",
        "\n",
        "Let's use LSTM instead of vanilla RNN and compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFhlKo7-qNhK"
      },
      "source": [
        "Plot the loss function of the number of epochs. Does the final loss become better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QtQy72O7Tz2"
      },
      "source": [
        "Теперь создадим двухслойную LSTM. На вход теперь будем подавать случайные последовательности из всего текста определенной длины. Это позволит получать батчи, полностью наполненные буквами, без паддингов. На выходе получаем предсказанную последовательность из букв. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXAa1CoSG5Qh"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4HxOSdDiNDu"
      },
      "source": [
        "poems_all = \"\"\n",
        "for s in poems:\n",
        "  poems_all += s\n",
        "  poems_all += \"\\n\\n\""
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5NSrbNQnqNhK"
      },
      "source": [
        "class nnLSTM(nn.Module):\n",
        "   def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim, \n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                            hidden_dim, \n",
        "                            num_layers=n_layers) \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(self.hidden_size, output_dim)\n",
        "\n",
        "        \n",
        "\n",
        "   def forward(self, text, hidden):\n",
        "        # text shape: (sent len, batch size)\n",
        "        \n",
        "        # Pass text through the embedding layer:\n",
        "        embedded = self.embedding(text).squeeze(2)\n",
        "        #embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "        # embedded shape: (sent len, batch size, emb dim)\n",
        "\n",
        "        # Pass embeddings through the LSTM:\n",
        "        outputs, (hidden, cell) = self.lstm(embedded, hidden)\n",
        "        # Outputs hold the backward and forward hidden states\n",
        "        # in the final layer.\n",
        "        # Hidden and cell are the backward and forward hidden and cell states\n",
        "        # at the final time-step.\n",
        "        # output shape: (sent len, batch size, hid dim * n directions)\n",
        "        # hidden/cell shape: (n layers * n directions, batch size, hid dim)\n",
        "        #we use our outputs to make a prediction of what the tag should be\n",
        "        text = self.fc(self.dropout(outputs))\n",
        "        # predictions shape: (sent len, batch size, output dim)\n",
        "\n",
        "        return text, (hidden, cell)\n",
        "   def init_hidden(self, batch_size=1):\n",
        "       return (torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device),\n",
        "               torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device))    "
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZPBfwlY-k0v"
      },
      "source": [
        "Размер последовательности возьмем равным 250 символам. Размер батча - 16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvcPHJdpbNHA"
      },
      "source": [
        "SAMPLE_LEN = 250\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def create_batch(sequence):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for _ in range(BATCH_SIZE):\n",
        "        start = np.random.randint(0, len(sequence) - SAMPLE_LEN)\n",
        "        seq_cur = sequence[start: start + SAMPLE_LEN]\n",
        "        input = torch.LongTensor(seq_cur[:-1]).view(-1, 1)\n",
        "        target = torch.LongTensor(seq_cur[1:]).view(-1, 1)\n",
        "        inputs.append(input)\n",
        "        targets.append(target)\n",
        "    return torch.stack(inputs, dim=0), torch.stack(targets, dim=0)"
      ],
      "execution_count": 387,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyBMlBuNDeMZ"
      },
      "source": [
        "Берем следующие гиперпараметры , размер embedding слоя - количество букв в алфавите, размер скрытого слоя - 200, dropout слой с коэффициентом 0.25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxLg0WkIbMZk",
        "outputId": "9be695f7-9df4-41bf-fd93-56c0345b3fe9"
      },
      "source": [
        "INPUT_DIM = num_tokens\n",
        "EMBEDDING_DIM = num_tokens\n",
        "HIDDEN_DIM = 200\n",
        "OUTPUT_DIM = num_tokens\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = False\n",
        "DROPOUT = 0.25\n",
        "PAD_IDX = token_to_id[PAD]\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model = nnLSTM(INPUT_DIM,\n",
        "                        EMBEDDING_DIM,\n",
        "                        HIDDEN_DIM,\n",
        "                        OUTPUT_DIM,\n",
        "                        N_LAYERS,\n",
        "                        BIDIRECTIONAL,\n",
        "                        DROPOUT,\n",
        "                        PAD_IDX)\n",
        "model.to(device)"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nnLSTM(\n",
              "  (embedding): Embedding(87, 87, padding_idx=86)\n",
              "  (lstm): LSTM(87, 200, num_layers=2)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              "  (fc): Linear(in_features=200, out_features=87, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXcGrOAPDdG4"
      },
      "source": [
        "Запускаем процесс обучения модели. Пользуемся опять же оптимизатором Adam, SGD и шедьюлером ReduceLROnPlateau."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLuEfA4-4ORU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "45c8162a-1e36-4f22-8495-c1e453356d2e"
      },
      "source": [
        "\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, amsgrad=True)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.000001, momentum=0.9)\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "#    optimizer, \n",
        "#    patience=6, \n",
        "#    verbose=True, \n",
        "#    factor=0.5\n",
        "#)\n",
        "\n",
        "n_epochs = 10000\n",
        "#loss_avg = []\n",
        "#sequence = np.array([token_to_id[char] for char in poems_all])\n",
        "#history = []\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    model.train()\n",
        "    train, target = get_batch(sequence)\n",
        "    train = train.permute(1, 0, 2).to(device)\n",
        "    target = target.permute(1, 0, 2).to(device)\n",
        "    hidden = model.init_hidden(BATCH_SIZE)\n",
        "\n",
        "    output, hidden = model(train, hidden)\n",
        "    loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    history.append(loss.item())\n",
        "    \n",
        "    loss_avg.append(loss.item())\n",
        "\n",
        "    if (i+1)%500==0:\n",
        "          with open(f\"/content/drive/MyDrive/MADE/semester2/NLP/char_lstm_onegin{history[-1]}.pth\", \"wb\") as fp:\n",
        "                 torch.save(model.state_dict(), fp)\n",
        "\n",
        "    if len(loss_avg) >= 50:\n",
        "        mean_loss = np.mean(loss_avg)\n",
        "        #print(f'Loss: {mean_loss}')\n",
        "        scheduler.step(mean_loss)\n",
        "        loss_avg = []\n",
        "        #model.eval()\n",
        "        #predicted_text = evaluate(model, char_to_idx, idx_to_char)\n",
        "        #print(predicted_text)\n",
        "\n",
        "        clear_output(True)\n",
        "        plt.plot(history,label='loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JCggmtxD_3A"
      },
      "source": [
        "На графике видно, когда лосс перестает убывать, включается шедьюлер и уменьшает шаг. В результате видим небольшую ступеньку и уменьшение лосса."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdvYWggCEQJD"
      },
      "source": [
        "На этот раз обучаем модель до значения лосса 0.24. Видим, что нам удалось  получить мЕньшее значение лосса, чем в vanilla-RNN на бОльшей длине последовательности, на которой учимся. Это связано еще и с тем, что в LSTM эффект затухания градиента не так явно выражен, как в RNN. В результате получаем минимальное значение лосса - 0.2421"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OL2X-DjYOC_",
        "outputId": "e55d165a-3f23-4886-d619-bdb8c61eb006"
      },
      "source": [
        "print(history[-1])"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2441912293434143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slJiNhcfYJDO"
      },
      "source": [
        "with open(f\"/content/drive/MyDrive/MADE/semester2/NLP/char_lstm_onegin{history[-1]}.pth\", \"wb\") as fp:\n",
        "                 torch.save(model.state_dict(), fp)\n"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_PbeJoWqNhL"
      },
      "source": [
        "Generate text using the trained net with different `temperature` parameter: `[0.1, 0.2, 0.5, 1.0, 2.0]`.\n",
        "\n",
        "Evaluate the results visually, try to interpret them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "tL4XtRDYqNhL"
      },
      "source": [
        "MAX_LENGTH = 300\n",
        "def generate_sample_lstm(model, seed_phrase='  when ', max_length= MAX_LENGTH, temperature=1.0):\n",
        "       \n",
        "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
        "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64).view(-1, 1, 1).to(device)\n",
        "    #hid_state = char_rnn.initial_state(batch_size=1)\n",
        "    \n",
        "    #feed the seed phrase, if any\n",
        "    #for i in range(len(seed_phrase) - 1):\n",
        "    #    print(x_sequence[:, -1].shape, hid_state.shape)\n",
        "    result = seed_phrase\n",
        "    hidden = model.init_hidden()\n",
        "    _, hidden = model(x_sequence, hidden)\n",
        "\n",
        "    inp = x_sequence[-1].view(-1, 1, 1)\n",
        "    \n",
        "    #start generating\n",
        "    for _ in range(max_length - len(seed_phrase)):\n",
        "        #print(x_sequence.shape, x_sequence)#, hid_state.shape)\n",
        "        #hid_state, out = char_rnn(x_sequence[:, -1], hid_state)\n",
        "        # Be really careful here with the model output\n",
        "        prediction, hidden = model(inp.to(device), hidden)\n",
        "        prediction_logits = prediction.cpu().data.view(-1)\n",
        "        #print(len(prediction_logits))\n",
        "        p_next = F.softmax(prediction_logits / temperature, dim=-1).detach().cpu().data.numpy()  \n",
        "        #print(len(p_next))    \n",
        "        \n",
        "        next_index = np.random.choice(len(tokens), p=p_next)\n",
        "        inp = torch.tensor([next_index], dtype=torch.int64).view(-1, 1, 1).to(device)\n",
        "        next_char = idx_to_token[next_index]\n",
        "        result  += next_char\n",
        "    result = result.replace(\"* \", \"  \")    \n",
        "    return result"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhsY1rfzqNhL"
      },
      "source": [
        "### Saving and loading models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdQsxCTlqNhL"
      },
      "source": [
        "Save the model to the disk, then load it and generate text. Examples are available [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html])."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_J4tTXzrQ08"
      },
      "source": [
        "Достанем модель с самым низким полученным значением loss = 0.2105.\n",
        "Как мне показалось, при таком значении лосса остаются слова, но смысл совсем ускользает. несуществующие слова перемежаются цельными фразами из текста.\n",
        "Чем больше температура, тем меньше фраз полностью из текста, больше разнообразия в фразах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M18cXgLIqTQn"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/MADE/semester2/NLP/char_lstm_onegin0.21057948470115662.pth\", \"rb\") as fp:\n",
        "    best_state_dict = torch.load(fp, map_location=\"cpu\")\n",
        "    model.load_state_dict(best_state_dict)\n"
      ],
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPvkz0WMG3Yr"
      },
      "source": [
        "Температура 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jV9AteiTqNhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "734642d4-2989-4489-c21f-db4986360b0f"
      },
      "source": [
        "x_sequence = generate_sample_lstm(model, seed_phrase='< мой дядя самых честных правил\\n', max_length= 1000, temperature=0.1)\n",
        "ind = x_sequence.find('>')\n",
        "print(x_sequence[0: ind + 1])\n"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< мой дядя самых честных правил\n",
            "  не отражает лукавил;\n",
            "  и очутил он был поэт.\n",
            "  в окнеи совсе их заслуждалась\n",
            "  когда б не правда ли?) пора!>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKmKXmqYG62G"
      },
      "source": [
        "Температура 0.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SttBG_TXM1BV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e77f08-ad92-4b86-8d9d-7a835861ac2f"
      },
      "source": [
        "x_sequence = generate_sample_lstm(model, seed_phrase='< как часто летнею порою\\n', max_length= 1000, temperature=0.2)\n",
        "ind = x_sequence.find('>')\n",
        "print(x_sequence[0: ind + 1])"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< как часто летнею порою\n",
            "  к нему слуга француз гильых\n",
            "  и после важно разошлись,\n",
            "  как будто делом занятья\n",
            "  он знал довольно отстал от время,\n",
            "  и сердцем плакать: воеврапила,\n",
            "  не видит никого, кто в предмете:\n",
            "  мы лучше поспешим на бал,\n",
            "  куда стремглав в ямской карете\n",
            "  уж мой онегин послушна,\n",
            "  в гостях улыбку возбуждал\n",
            "  своей осанкою сердечной\n",
            "  на толки промолеи на двери.\n",
            "  теперь солилась на свете;\n",
            "  на красноречив огражешь,\n",
            "  но не скажет, не всё равно.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0M9RUazG8i0"
      },
      "source": [
        "Температура 0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QgkY5Z9SO3o",
        "outputId": "fccb9894-e44b-456f-ce43-46291dcfbc1e"
      },
      "source": [
        "x_sequence = generate_sample_lstm(model, seed_phrase='< как часто летнею порою\\n', max_length= 1000, temperature=0.5)\n",
        "ind = x_sequence.find('>')\n",
        "print(x_sequence[0: ind + 1])"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< как часто летнею порою\n",
            "  давно нетерпеливо ждал;\n",
            "  меж тем, механик дедовей,\n",
            "  вдруг душе светлою руке,\n",
            "  я зарецкий зорочи двор,\n",
            "  когда он злую постеле\n",
            "  татьяну в это удили совершить\n",
            "  в тени друг другу в потом\n",
            "  в представить барать уверен,\n",
            "  дитя смело всегда новый\n",
            "  не смеет; мигом обедам\n",
            "  трактиров наших не потом\n",
            "  среди вседневно ждет она.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdGhpe_tHA1c"
      },
      "source": [
        "Температура 1.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KosExVTYT6un",
        "outputId": "bafb754b-c44d-4f03-fe98-7b28de97a042"
      },
      "source": [
        "x_sequence = generate_sample_lstm(model, seed_phrase='< как часто летнею порою\\n', max_length= 1000, temperature=1.0)\n",
        "ind = x_sequence.find('>')\n",
        "print(x_sequence[0: ind + 1])"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< как часто летнею порою\n",
            "  ко мне падушницы крестам,\n",
            "  и в шалаше и к ней, к ольге страсти\n",
            "  и эту прощают собое\n",
            "  своим обещал он ей страстей,\n",
            "  на смиренные на обуза\n",
            "  пред этой любви милой диной\n",
            "  вы, праздно был по прежнему полей,\n",
            "  когда, нужда и быть должны;\n",
            "  так ваш сердце утра потом\n",
            "  уже редеют лукавико,\n",
            "  и лиликов, супруга ей\n",
            "  они предрады души моя,\n",
            "  ему на шалах и свободы,\n",
            "  в гонит усталие и умхать\n",
            "  и смусный шких и лет.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjBbBMB5HEp0"
      },
      "source": [
        "Температура 2.0. При этой температуре слова создаются новые несуществующие слова (изчебражденье, счастлипелый)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_uO4pmFep6T",
        "outputId": "e4f1cc97-748b-442a-c29a-68fe6f16c62f"
      },
      "source": [
        "x_sequence = generate_sample_lstm(model, seed_phrase='< как часто летнею порою\\n', max_length= 1000, temperature=2.0)\n",
        "ind = x_sequence.find('>')\n",
        "print(x_sequence[0: ind + 1])"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< как часто летнею порою\n",
            "  к гойду вас пружины изчебражденье\n",
            "  на сил бы суросел счастлипелый\n",
            "  несчудиниться я в намару;\n",
            "  но вот и ты знайне ей,\n",
            "  а пиров издалеках.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyjzj7hvrNu8"
      },
      "source": [
        "Посмотрим модели с бОльшим лоссом. Хотя смысла в тексте все меньше, но иногда даже проскакивает какая-то рифма и как-будто больше сохраняется ритм стиха"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHMAdukXhuGd"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/MADE/semester2/NLP/char_lstm_onegin0.8114542961120605.pth\", \"rb\") as fp:\n",
        "    best_state_dict = torch.load(fp, map_location=\"cpu\")\n",
        "    model.load_state_dict(best_state_dict)"
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruh-I2J0kh1q",
        "outputId": "0d6d31d7-0c2c-4d1f-9611-a22f8f2acad3"
      },
      "source": [
        "x_sequence = generate_sample_lstm(model, seed_phrase='< от хладного разврата света\\n', max_length= 1000, temperature=0.5)\n",
        "ind = x_sequence.find('>')\n",
        "print(x_sequence[0: ind + 1])"
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< от хладного разврата света\n",
            "  ни крестится фарфон и вежда,\n",
            "  за перо, любовник ного брега,\n",
            "  пред этой невернован узон,\n",
            "  когда гремел мазурки толк!\n",
            "  как томен вид и грусти не,\n",
            "  о ножках вы, здесь и блистают думой.\n",
            "  она была на совелелей;\n",
            "  но видим готов со мной\n",
            "  поступила бараын посель.\n",
            "  пока него так наслаждений\n",
            "  одно талии своей.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdERMl21lIZ7",
        "outputId": "ec2369b1-c9c3-4c33-e117-0ae13693a61b"
      },
      "source": [
        "x_sequence = generate_sample_lstm(model, seed_phrase='< как часто летнею порою\\n', max_length= 1000, temperature=0.2)\n",
        "ind = x_sequence.find('>')\n",
        "print(x_sequence[0: ind + 1])"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< как часто летнею порою\n",
            "  косточкам в глуши степей доски.\n",
            "  она глядит ему в ладких,\n",
            "  с плеча вставал он уверен,\n",
            "  что мы начинает и досугом\n",
            "  открыла тайну, как супругом\n",
            "  свои трещит и любви порою;\n",
            "  в сумрашен боле тридебеньей\n",
            "  измены вас непричанье верной\n",
            "  средей прежней ветреный письму\n",
            "  и ужин, полдою, торной\n",
            "  на синих, для дома ждет поэт\n",
            "  татьяну в мольбе молодой,\n",
            "  моей душе мог и грудь\n",
            "  отворотив от пистолета.\n",
            "  теперь с котой раздался поним,\n",
            "  бежала за данья и светло.\n",
            "  татьяна вижу я за столом.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJqOcJTpvbEg"
      },
      "source": [
        "Еще увеличиваем значение лосса (0.931)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXzF1p-vlxer"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/MADE/semester2/NLP/char_lstm_onegin0.9313090443611145.pth\", \"rb\") as fp:\n",
        "    best_state_dict = torch.load(fp, map_location=\"cpu\")\n",
        "    model.load_state_dict(best_state_dict)\n"
      ],
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FqocdZvl4a6",
        "outputId": "cb9b3a20-dd8b-4922-9459-c9bba7e7d85f"
      },
      "source": [
        "x_sequence = generate_sample_lstm(model, seed_phrase='< как часто летнею порою\\n', max_length= 1000, temperature=0.5)\n",
        "ind = x_sequence.find('>')\n",
        "print(x_sequence[0: ind + 1])"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< как часто летнею порою\n",
            "  и крик и ласки и взор,\n",
            "  в глазах легкие с ним обезрев,\n",
            "  как он взял об наш и к тихо.\n",
            "  он позволял – не как-нибудь,\n",
            "  но в страшного страстей их света\n",
            "  быть может, а под ветернея.\n",
            "  поесь веду он злой апуть.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5boz1BImOU5",
        "outputId": "89b49d89-41ff-441e-cf02-ff3fa86009bb"
      },
      "source": [
        "x_sequence = generate_sample_lstm(model, seed_phrase='< как часто летнею порою\\n', max_length= 1000, temperature=0.2)\n",
        "ind = x_sequence.find('>')\n",
        "print(x_sequence[0: ind + 1])"
      ],
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< как часто летнею порою\n",
            "  к ней на вала записных дам\n",
            "  и утро нашей сердце ей.\n",
            "  он ее в постели подал\n",
            "  без молкливо требепный пал…\n",
            "  ото всего, что сё верелый слов,\n",
            "  довольно скучен высший тон;\n",
            "  а между тем припомнил светают:\n",
            "  хоть он мне полно, полилось». —\n",
            "  «да кто ж она?» – «жена моя».>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-A00oWfoJP_",
        "outputId": "000cab7a-aa75-4fca-8116-6ad9df0b0d98"
      },
      "source": [
        "x_sequence = generate_sample_lstm(model, seed_phrase='< как часто летнею порою\\n', max_length= 1000, temperature=0.3)\n",
        "ind = x_sequence.find('>')\n",
        "print(x_sequence[0: ind + 1])"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< как часто летнею порою\n",
            "  к ней не домчится гимн времен,\n",
            "  благословение племен.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghYDXVLxo2c6",
        "outputId": "603d1fed-4ace-410b-eecd-01fec5d79cce"
      },
      "source": [
        "x_sequence = generate_sample_lstm(model, seed_phrase='< так думал молодой повеса,\\n', max_length= 1000, temperature=0.1)\n",
        "ind = x_sequence.find('>')\n",
        "print(x_sequence[0: ind + 1])"
      ],
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< так думал молодой повеса,\n",
            "  увы! любовник молодой,\n",
            "  блистал фонвизин, друг свободы,\n",
            "  и путешествия ему,\n",
            "  как верхом на вспомнил он;\n",
            "  в санки он оставил кал,\n",
            "  в душе твой гости молодой.\n",
            "  все дочек хранивет предметы\n",
            "  мне страдал бы собою\n",
            "  сведет еще уж давно ного:\n",
            "  никто будет старинных лет,\n",
            "  безмолвно буду я зевать\n",
            "  и дружеский солненье сень:\n",
            "  то не имев угралость, да должны\n",
            "  в долг осушать бутылки три.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU3gKD0kmh3K"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/MADE/semester2/NLP/char_lstm_onegin1.0113763809204102.pth\", \"rb\") as fp:\n",
        "    best_state_dict = torch.load(fp, map_location=\"cpu\")\n",
        "    model.load_state_dict(best_state_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6A3eit8mcWk",
        "outputId": "3d3049f7-95b7-41ed-df8f-2c030b8ec46c"
      },
      "source": [
        "x_sequence = generate_sample_lstm(model, seed_phrase='< мы все учились понемногу\\n', max_length= 1000, temperature=0.2)\n",
        "ind = x_sequence.find('>')\n",
        "print(x_sequence[0: ind + 1])\n"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< мы все учились понемногу\n",
            "  чему-ни труда душе бей,\n",
            "  дружно стал бы бледник сество;\n",
            "  под небом шиллера и гете.\n",
            "  он отличился, сходить с франту\n",
            "  и нашей братье рифмачам\n",
            "  крестила дряхлою растка:\n",
            "  в той кучка холя тихонька:\n",
            "  верней наши головою,\n",
            "  того, что от услага не хотел\n",
            "  себе влюбляясь отворилась,\n",
            "  улыбка слез, тому же лел.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McfYaHDL848n"
      },
      "source": [
        "Вывод : LSTM сеть гораздо легче оптимизируется, чем RNN. Посколько имеет skip обходной слой, в котором градиент меньше затухает.\n",
        "Другое дело, что на практике мне показалось стихи с бОльшим значеним лосса в LSTM, больше похожи именно на стихи, с рифмой и ритмом. Но все еще очень далеки от идеала :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tChWkw1qNhM"
      },
      "source": [
        "### References\n",
        "1. <a href='http://karpathy.github.io/2015/05/21/rnn-effectiveness/'> Andrew Karpathy blog post about RNN. </a> \n",
        "There are several examples of genration: Shakespeare texts, Latex formulas, Linux Sourse Code and children names.\n",
        "2. <a href='https://github.com/karpathy/char-rnn'> Repo with char-rnn code </a>\n",
        "3. Cool repo with PyTorch examples: [link](https://github.com/spro/practical-pytorch`)"
      ]
    }
  ]
}