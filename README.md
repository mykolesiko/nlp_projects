# nlp_projects



1) Poetry generation using RNNs in style of Pushkin.[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mykolesiko/nlp_projects/blob/master/Poetry_generation/Lab01_Poetry_generation_2.ipynb)

2) The the EN-RU translation task by different used approachs:   
      using RNNs as encoder and decoder with attention mechanism.[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mykolesiko/nlp_projects/blob/master/Neural_Machine_Translation/Lab2_attention.ipynb)   
      CNN encoder (with or without positional encoding), with pretraining the language model [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mykolesiko/nlp_projects/blob/master/Neural_Machine_Translation/Lab2_convolution_position.ipynb)     
      using transformers  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/mykolesiko/nlp_projects/blob/master/Neural_Machine_Translation/Lab2_transformers.ipynb)   
      
3) enhance one the models in previous item using Self-critical Sequence Training  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mykolesiko/nlp_projects/blob/master/Self-critical_Sequence_Training/Lab2_attention_RL_send.ipynb)   
    look at https://arxiv.org/abs/1612.00563   
4) Toxic text or not 
https://colab.research.google.com/drive/1ke2kJCjv9RHaTw1hAAtX9n_YiO5A1ybs?usp=sharing    


